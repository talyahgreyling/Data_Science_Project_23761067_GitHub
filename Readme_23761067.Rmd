---
title: "Readme (23761067)"
author: "Talyah Greyling" 
output: 
    html_document:
        css: styles.css
---

### Goals
    (1) Transform and clean data
    (2) Exploratory data description of wine review data 
    (3) Statistical modeling of wine review data

### Setup defaults         
```{r setup}

knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      cache = FALSE,
                      fig.align = "center",
                      fig.pos = "H", 
                      fig.width = 6, 
                      fig.height = 5) 
                      
```

### Housekeeping
```{r housekeeping}

# Clear environment
rm(list = ls())           

# Load necessary packages
pacman::p_load(bonsai,
               broom,
               dbbasic, 
               dplyr,
               ggplot2,
               ggwordcloud,
               glue,
               here, 
               kableExtra, 
               knitr,
               ldatuning,                  
               modeltime,
               pacman,
               pdp,
               quanteda,
               randomForest,
               readr,
               remotes,
               stopwords,
               stringr,
               targets,
               tibble,
               tidymodels,
               tidyr,
               tidytext, 
               tidyverse,
               topicmodels,
               vip,
               wordcloud,
               xtable)

# Suppress scientific notation  
options(scipen = 999)

# Reproducibility
set.seed(777)

# Set theme & colour palette 
palette <- c("#4d8049", "#3A7370", "#FFC57B", "#FB9072", "#CE677A", "#9c0315")

# Import data
data <- readRDS("data/calitzdorp.rds")
               
```

## Goal (1): Transform and clean data

### Explore dataset
```{r explore data}

data %>%
    sample_n(1) %>% 
    t

```

#### Table 1: Description of variables utilised for analysis
```{r tab1, results = "asis"}

# Create and populate data frame
df1 <- data.frame(
    Variable = c("id", "rating", "note", "language"), 
    Description = c("Identification code of review",
                    "Rating out of 5 assigned to the wine by the reviewer",
                    "Note written about the wine by the reviewer",
                    "Language of the review"))

table1 <- print(xtable(df1),
                    type = "html",
                    include.rownames = TRUE,  
                    print.results = FALSE)     
table1

```

### Preparing dataset for analysis:
* Started with 2298 observations of 107 variables 
* Extracted the 3 variables useful for my research question 
* Only English observations were kept as using APIs for translation is too costly (1667 observations remaining)
* Unnested tokens (30 800 tokens were recorded)
* I followed the standard pre-processing steps of removing punctuation, numbers, symbols, characters and taking everything to lower case
* I also used the default tidyverse stop_words dataset to remove stop words (16 869 tokens remaining) using 3 respective lexicons: snowball (from the tm package), onix (from the ONIX text retrieval system) and smart (from the SMART information retrieval system). 
* All clean tokes were used in the random forrest to avoid unnecessary sampling bias. 
```{r clean tokens}

# Get review data in clean token format 
review_tokens <- data %>%
    filter(str_detect(language, "en")) %>% # filter for only English entries
    select(c(review_id = "id", "rating", "note")) %>%  # extract useful columns 
    unnest_tokens(input = note, 
                  output = word,
                  to_lower = TRUE,
                  strip_punct = TRUE,
                  strip_numeric = TRUE) %>%  
    mutate(word = gsub("_", "", word)) %>% 
    mutate(word = str_extract(word, "[[:alpha:]]+")) %>% 
    anti_join(stop_words, by = join_by(word)) # remove stop words
 
```

## Goal (2): Exploratory data description of wine review data:
* Plot rating frequency of tokens 
* Plot wordcloud of popular words

### Average rating counts
```{r averating}

# Calculate the average rating for each word and its count
words_ratings_counts <- review_tokens %>%
    group_by(word) %>%
    summarise(
        ave_rating = mean(rating, na.rm = TRUE),
        word_count = n(),
        .groups = "drop"
    )
    
```

#### Figure1: Histogram for frequency of average ratings of review tokens
```{r histogram}

# Plot histogram
fig1 <- ggplot(words_ratings_counts, aes(x = ave_rating)) +
  geom_histogram(aes(y = ..count..), binwidth =0.5, bins = 30, fill = "#3A7370", color = "black", alpha = 1) +
  theme_minimal() +
  labs(title = "Histogram of average ratings of review tokens",
       x = "Average Rating", 
       y = "Frequency") 
fig1

ggsave("writeup/fig1.png", plot = fig1, width = 6, height = 5, dpi = 300)

```

### Popular tokens
```{r popular}

# Create an index that determines popular words
words_popular <- words_ratings_counts %>%
    mutate(popularity_index = 0.8 * word_count + 0.2 * ave_rating) %>%
    filter(
        word_count > 10,
        word != "",!is.na(word),!is.na(ave_rating),!is.na(word_count),!is.na(popularity_index)
    ) %>%
    filter(!word %in% stop_words) %>%
    arrange(desc(popularity_index)) %>%
    select(c(word, popularity_index))

```

#### Figure 2: Word cloud of top 50 popular words in reviews based on index 
```{r index wordcloud}

# Get top 50 words
top50 <- words_popular %>%
  slice_max(order_by = popularity_index, n = 50)

# Set palette 
top50$assigned_color <- sample(palette, nrow(top50), replace = TRUE)

# Create Word Cloud
fig2 <- ggplot(top50, aes(label = word, size = popularity_index, color = assigned_color)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 40) +
  scale_color_identity() +
  theme_minimal()
fig2 

ggsave("writeup/fig2.png", plot = fig2, width = 6, height = 5, dpi = 300)


```

## Goal (3): Statistical modeling of wine review data
* Use 2 step modelling process 
* Step 1: Use RandomForest as a variable selection method (words as columns & ratings as rows) to determine the top 20 words 
    * Plot training & testing data to confirm similar ratings distributions
    * Plot RF performance with different mtry values
    * Visualise top 20 with a variable importance plot 
    * Visualise top 50 in a wordcloud
* Step 2: run an OLS regression of the best words to get coefficients for their probability of determining good ratings

### Step 1: Random Forest variable selection
```{r randomforest}

# Convert data to DTM format: 
dtm <- review_tokens %>%
    group_by(review_id, word) %>% 
    summarise(count = n(), .groups = "drop") %>% 
    pivot_wider(names_from = word,
                values_from = count, 
                values_fill = list(count = 0))

# Add back rating & drop review_id
ratings <- review_tokens %>% 
    select(review_id, rating) %>% 
    distinct()
dtm <- left_join(ratings, dtm, by = "review_id") %>% 
    select(-review_id)

# Define training and testing sets for prediction
split_ratings <- initial_split(dtm, prop = 0.7)
training_ratings <- training(split_ratings)
testing_ratings <- testing(split_ratings)

``` 

#### Figure 3: Distribution of training & testing data
```{r traintest distribution}

# Confirm that training & testing data have similar ratings distributions
fig3 <- ggplot() +
        geom_density(data = training_ratings, aes(rating.x), linewidth = 1.5, alpha = 0.5, color = "#CE677A") +
        geom_density(data = testing_ratings, aes(rating.x), linewidth = 1.5, alpha = 0.5, color = "#3A7370") +
        scale_x_continuous(labels = scales::comma) +
        theme_minimal() +
        labs(title = "Ratings Distribution: Train (Red) vs Test (Green)", x = "Rating", y = "Density")
fig3

ggsave("writeup/fig3.png", plot = fig3, width = 6, height = 5, dpi = 300)

``` 

```{r randomforest continued}

# RandomForest recipe
wine_rec <- recipe(rating.x ~ ., data = training_ratings) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())

# RandomForest specification model
model_spec <- rand_forest(
  mtry = tune(),       
  min_n = 10,           
  trees = 100           
  ) %>%
  set_engine("ranger", num.threads = 10, 
             importance = "permutation") %>%  
  set_mode("regression")

# Inspect the low level call 
model_spec %>%  
    translate()

# Building the workflow
(model_workflow <- workflow() %>% 
   add_recipe(wine_rec) %>% 
   add_model(model_spec))

# Cross validation
(cv_folds <- vfold_cv(training_ratings, v = 5, repeats = 1))

# Set regular grid
grid_vals <- grid_regular(mtry(range = c(10, 18)), levels = 5)

model_res <- model_workflow %>% 
  tune_grid(
    resamples = cv_folds,
    grid = grid_vals,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(rmse)
  )
```

#### Figure 4:
```{r mtry}

# Evaluate how RF performs with different mtry values
fig4 <- model_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = mtry, y = mean)) +
  geom_point() + 
  geom_line() +
  facet_wrap(~.metric, scales = "free_y") +
  labs(
    title = "Random Forest Performance by mtry",
    x = "mtry (number of variables tried at each split)",
    y = "Mean metric value (lower = better)",
    color = "mtry"
  ) +
  theme_minimal()
fig4

ggsave("writeup/fig4.png", plot = fig4, width = 6, height = 5, dpi = 300)

```

```{r randomforest continued again}

# final fit and evaluation
best_params <- model_res %>% 
  show_best(metric = "rmse") %>% 
  slice(1)

# Plug optimal hyperparameters into workflow & fit on all training data 
(final_model <- finalize_workflow(model_workflow, best_params))

final_fit <- final_model %>%
  fit(data = training_ratings)

# Inspect trained model
final_fit %>% 
  extract_fit_parsnip()

# Asses how model performs on unseen data
metricsdata <- predict(final_fit, testing_ratings) %>%
  bind_cols(testing_ratings) %>%
  metrics(truth = rating.x, estimate = .pred)
```

#### Table 2: Model Performance Metrics
```{r tab2, results = "asis"}
xtab_metrics <- xtable(metricsdata,
                       caption = "Model Performance Metrics",
                       digits = c(0, 0, 4, 4))

table2 <- print(xtable(xtab_metrics),
                    type = "html",
                    include.rownames = FALSE,  
                    caption.placement = "top",
                    print.results = FALSE) 
table2

```

#### Figure 5: Variable Importance Plot of top 20 words in reviews identified by Random Forest model 
```{r vip, fig.length = 10}

# Which features had the most influence on predictions 
fitted_model <- final_fit %>% 
    extract_fit_parsnip()
varimp <- vi(fitted_model)
varimp

# Draw VIP
fig5 <- vip(fitted_model, num_features = 20, geom = "col", 
    aesthetics = list(fill = "#CE677A")) +
    labs(
    title = "Variable Importance Plot of Top 20 Words Identified by Random Forest") +
  theme_minimal() 
fig5

ggsave("writeup/fig5.png", plot = fig5, width = 6, height = 5, dpi = 300)

```

#### Figure 6: Word cloud of top 50 words in reviews identified by Random Forest model
```{r rf wordcloud}

# Convert data for wordcloud
df_wordcloud <- vi(fitted_model) %>%
  arrange(desc(Importance)) %>%
  slice(1:50) %>% 
    select(word = Variable, freq = Importance) 

# Set palette 
df_wordcloud$assigned_color <- sample(palette, nrow(df_wordcloud), replace = TRUE)    

# Create Word Cloud
fig6 <- ggplot(df_wordcloud, aes(label = word, size = freq, color = assigned_color)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 40) +
  scale_color_identity() +
  theme_minimal()
fig6

ggsave("writeup/fig6.png", plot = fig6, width = 6, height = 5, dpi = 300)

```

### Step 2: OLS regression 
```{r regression}

# Filter dtm by top 20 words determined by RF
top_20 <- vip(fitted_model, num_features = 20)$data$Variable
df_ols <- dtm %>% 
  select(rating.x, all_of(top_20))

ols_model <- lm(rating.x ~ ., data = df_ols)
summary(ols_model)

```

#### Table 3: OLS Regression Results (Top 20 Words)
```{r tab3, results = "asis"}
ols_table <- xtable(ols_model, 
                    caption = "OLS Regression Results (Top 20 Words)")

table3 <- print(ols_table,
      type = "html",
      include.rownames = TRUE,
      caption.placement = "top",
      html.table.attributes = 'class="table table-striped" style="width:100%;"')
table3

```

### Visualising OLS regression
```{r ols wrangling}

# Tidy results
ols_results <- broom::tidy(ols_model) %>%
  arrange(desc(estimate))

# Filter out intercept and format
final_results <- ols_results %>%
  filter(term != "(Intercept)") %>%
  select(term, estimate, p.value) %>%
  mutate(
    effect = ifelse(estimate > 0, "Positive", "Negative"),
    term = fct_reorder(term, estimate)
  )
final_results

```

#### Figure 7: Plot of coefficients on OLS regression of top 20 words in reviews identified by RandomForest model
```{r fig7 OLSplot}

fig7 <-  ggplot(final_results, aes(x = estimate, y = term, fill = effect)) +
  geom_col() +
  scale_fill_manual(values = c("#9c0315", "#4d8049")) +
  labs(
    title = "Words with Strongest Correlation to Wine Ratings",
    x = "Effect on Rating",
    y = "Word"
  ) +
  theme_bw()
fig7

ggsave("writeup/fig7.png", plot = fig7, width = 6, height = 5, dpi = 300)
  
```















